{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-05-13T20:50:40.341615Z","iopub.status.busy":"2023-05-13T20:50:40.341203Z","iopub.status.idle":"2023-05-13T20:50:44.968488Z","shell.execute_reply":"2023-05-13T20:50:44.967254Z","shell.execute_reply.started":"2023-05-13T20:50:40.341583Z"}},"source":["# How to create a HDF5 Dataset\n","Hey guys! As already mentioned in the discussions <br>\n","( https://www.kaggle.com/competitions/google-research-identify-contrails-reduce-global-warming/discussion/409401 )<br>\n",", **HDF5** ist a great data format to manage such huge datasets and achieve fast read access.\n","\n","That's why I wrote some code to convert the dataset to HDF5. As I couldn't find a way to upload datasets of this size (300+GB) on kaggle, this notebook only processes the first 2500 instances as an example. I'm currently working on finding a nice way to share the full HDF5-file.\n","<br><br>\n","I also want to mention that this is V1, which means **it is only a prototype**. I'm not an HDF5 expert myself, and I will constantly improve this version. So if you have any advice, **feel free to help!!!**\n","Some things I want to implement soon are for example parallel processing, fundamental preprocessing, and a better dataloader. \n","<br><br>\n","To demonstrate the perfomance speedup, I used this Pytorch dataloader: <br>\n","https://www.kaggle.com/code/thomasrochefort/pytorch-dataloader-example\n","<br>So **check out this notebook** for more details about this part!!\n"]},{"cell_type":"markdown","metadata":{},"source":["### Versions: \n","V1: <br>- 2x faster with num_workers = 1 <br>\n","    - Problems with num_workers = 4, almost no speedup <br>\n","    (Probably the bottleneck is opening the hdf5-file) <br>\n","    \n","V2: <br>- Now dtype float16 is used since we don't lose much information, as mentioned in discussions <br>\n","    - hdf5-file isn't opened in the get_item method but in the constructor. <br>\n","    - Much faster with num_workers = 1 <br>\n","    - 1.5x faster with num_workers = 4"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting h5py\n","  Downloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.0)\n","Installing collected packages: h5py\n","Successfully installed h5py-3.9.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install h5py"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:18:07.754505Z","iopub.status.busy":"2023-05-14T10:18:07.753843Z","iopub.status.idle":"2023-05-14T10:18:07.762153Z","shell.execute_reply":"2023-05-14T10:18:07.761087Z","shell.execute_reply.started":"2023-05-14T10:18:07.754441Z"},"trusted":true},"outputs":[],"source":["# Imports\n","import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import h5py\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:18:10.093139Z","iopub.status.busy":"2023-05-14T10:18:10.092035Z","iopub.status.idle":"2023-05-14T10:18:10.111796Z","shell.execute_reply":"2023-05-14T10:18:10.110453Z","shell.execute_reply.started":"2023-05-14T10:18:10.093051Z"},"trusted":true},"outputs":[],"source":["BASE_DIR = \"/kaggle/input/google-research-identify-contrails-reduce-global-warming\"\n","HDF_DIR = \"/kaggle/working/dataset_train/hdf5/\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:18:10.11442Z","iopub.status.busy":"2023-05-14T10:18:10.113275Z","iopub.status.idle":"2023-05-14T10:18:10.148691Z","shell.execute_reply":"2023-05-14T10:18:10.147398Z","shell.execute_reply.started":"2023-05-14T10:18:10.114368Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["num train samples: \t\t20529\n","num validation samples: \t1856\n","num test samples: \t\t2\n"]}],"source":["train_ids = os.listdir(BASE_DIR +'/'+ \"train\")\n","val_ids = os.listdir(BASE_DIR +'/'+ \"validation\")\n","test_ids = os.listdir(BASE_DIR +'/'+ \"test\")\n","\n","print(f\"num train samples: \\t\\t{len(train_ids)}\")\n","print(f\"num validation samples: \\t{len(val_ids)}\")\n","print(f\"num test samples: \\t\\t{len(test_ids)}\")"]},{"cell_type":"markdown","metadata":{},"source":["**Note:** Every Instance saves arrays for band_08-16 and two arrays for the \"ground truth\". <br>\n","Validation instances dont have human_individual_masks.npy and test instances obviously don't have ground truth arrays:"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:18:10.185598Z","iopub.status.busy":"2023-05-14T10:18:10.184468Z","iopub.status.idle":"2023-05-14T10:18:10.196199Z","shell.execute_reply":"2023-05-14T10:18:10.195092Z","shell.execute_reply.started":"2023-05-14T10:18:10.185561Z"},"trusted":true},"outputs":[],"source":["def saveToHDF5(id_, type_, group):\n","    if type_ not in [\"train\", \"validation\", \"test\"]:\n","        raise ValueError(\"type has do be one of ['train', 'validation', 'test']\")\n","    \n","    instance = group.create_group(id_)\n","    path = BASE_DIR + f\"/{type_}/\" + id_\n","    \n","    bands_data = []\n","    for i in range(8, 17):\n","        band = f\"band_{str(i).zfill(2)}\"\n","        array = np.load(f\"{path}/{band}.npy\")\n","        bands_data.append(array)\n","    bands_data = np.stack(bands_data, axis=-1).astype(np.float16) \n","    instance.create_dataset(\"bands_data\", data=bands_data)\n","    \n","    # Only train and validation have human_pixel_masks\n","    if type_ in [\"train\", \"validation\"]:\n","        agg_masks = np.load(path + \"/human_pixel_masks.npy\").astype(np.float16)\n","        instance.create_dataset(\"human_pixel_masks\", data=agg_masks)\n","    if type_ == \"train\":\n","        ind_masks = np.load(path + \"/human_individual_masks.npy\").astype(np.float16)\n","        instance.create_dataset(\"human_individual_masks\", data = ind_masks)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:18:10.200901Z","iopub.status.busy":"2023-05-14T10:18:10.200145Z","iopub.status.idle":"2023-05-14T10:31:31.513464Z","shell.execute_reply":"2023-05-14T10:31:31.511936Z","shell.execute_reply.started":"2023-05-14T10:18:10.200853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing Training Data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 20529/20529 [54:14<00:00,  6.31it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","Processing Validation Data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1856/1856 [04:47<00:00,  6.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Processing Test Data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:00<00:00,  6.60it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Finished all jobs!\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Create the HDF5-file\n","with h5py.File(HDF_DIR + \"/Google_Contrails.hdf5\", \"w\") as f:\n","    \n","    print(\"Processing Training Data...\")\n","    train = f.create_group(\"train\")\n","    for train_id in tqdm(train_ids):\n","        saveToHDF5(train_id, \"train\", train)\n","        \n","    print(\"\\nProcessing Validation Data...\")\n","    validation = f.create_group(\"validation\")\n","    for val_id in tqdm(val_ids):\n","        saveToHDF5(val_id, \"validation\", validation)\n","    \n","    print(\"\\nProcessing Test Data...\")\n","    test = f.create_group(\"test\")\n","    for test_id in tqdm(test_ids):\n","        saveToHDF5(test_id, \"test\", test)\n","        \n","    print(\"\\nFinished all jobs!\")  "]},{"cell_type":"markdown","metadata":{},"source":["## Now let's compare the speedup!<br>\n","As mentioned earlier, I'm comparing the speedup by iterating over a dataloader once. I used this dataloader:<br>\n","https://www.kaggle.com/code/thomasrochefort/pytorch-dataloader-example \n","<br>\n","For the HDF5-Dataloader, I had to make some small adjustments."]},{"cell_type":"markdown","metadata":{},"source":["### 1) No HDF5"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:41:44.604357Z","iopub.status.busy":"2023-05-14T10:41:44.603719Z","iopub.status.idle":"2023-05-14T10:41:44.620368Z","shell.execute_reply":"2023-05-14T10:41:44.619154Z","shell.execute_reply.started":"2023-05-14T10:41:44.604285Z"},"trusted":true},"outputs":[],"source":["class ContrailDataset(Dataset):\n","    def __init__(self, base_dir, data_type='train', transform=None):\n","        assert data_type in ['train', 'validation', 'test'], \\\n","            \"'data_type' should be one of 'train', 'validation', or 'test'\"\n","\n","        self.base_dir = base_dir\n","        self.data_type = data_type\n","        self.transform = transform\n","        self.record = os.listdir(self.base_dir +'/'+ self.data_type)\n","\n","    def __len__(self):\n","        return len(self.record)\n","\n","    def __getitem__(self, idx):\n","        record_id = self.record[idx]\n","        record_dir = os.path.join(self.base_dir, self.data_type, record_id)\n","\n","        bands_data = []\n","        for i in range(8, 17):\n","            band_file = os.path.join(record_dir, f'band_{str(i).zfill(2)}.npy')\n","            band_data = np.load(band_file)\n","            bands_data.append(band_data)\n","        bands_data = np.stack(bands_data, axis=-1)\n","\n","        if self.data_type in ['train', 'validation']:\n","            pixel_masks_file = os.path.join(record_dir, 'human_pixel_masks.npy')\n","            pixel_masks = np.load(pixel_masks_file)\n","        else:\n","            pixel_masks = None\n","        return bands_data, pixel_masks\n","\n","\n","def get_dataloader(base_dir, data_type, batch_size, transform=None):\n","    dataset = ContrailDataset(base_dir, data_type=data_type, transform=transform)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers = 4)\n","    return dataloader"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:41:44.787516Z","iopub.status.busy":"2023-05-14T10:41:44.786744Z","iopub.status.idle":"2023-05-14T10:41:44.980975Z","shell.execute_reply":"2023-05-14T10:41:44.979869Z","shell.execute_reply.started":"2023-05-14T10:41:44.787451Z"},"trusted":true},"outputs":[],"source":["train_dataloader = get_dataloader(BASE_DIR, 'train', batch_size=16)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:41:45.266692Z","iopub.status.busy":"2023-05-14T10:41:45.265384Z","iopub.status.idle":"2023-05-14T10:53:12.985666Z","shell.execute_reply":"2023-05-14T10:53:12.98414Z","shell.execute_reply.started":"2023-05-14T10:41:45.266643Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 212/1284 [02:01<10:13,  1.75it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m bands, masks \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m      2\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n","File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n","File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n","File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n","File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n","File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for bands, masks in tqdm(train_dataloader):\n","    continue"]},{"cell_type":"markdown","metadata":{},"source":["### 2) With HDF5"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:57:34.372894Z","iopub.status.busy":"2023-05-14T10:57:34.371435Z","iopub.status.idle":"2023-05-14T10:57:34.387213Z","shell.execute_reply":"2023-05-14T10:57:34.385709Z","shell.execute_reply.started":"2023-05-14T10:57:34.372838Z"},"trusted":true},"outputs":[],"source":["class ContrailDatasetHDF5(Dataset):\n","    def __init__(self, hdf5_file, data_type='train', transform=None):\n","        assert data_type in ['train', 'validation', 'test'], \\\n","            \"'data_type' should be one of 'train', 'validation', or 'test'\"\n","\n","        self.hdf5_file = hdf5_file\n","        self.data_type = data_type\n","        self.transform = transform\n","        f = h5py.File(hdf5_file, \"r\")\n","        self.group = f[self.data_type]\n","        self.records = list(self.group.keys())[:2000]\n","\n","    def __len__(self):\n","        return len(self.records)\n","\n","    def __getitem__(self, idx):\n","        record_id = self.records[idx]\n","        \n","        bands = self.group[f\"{record_id}/bands_data\"][:,:,:,:]\n","        # print(bands.shape)\n","        if self.data_type in ['train', 'validation']:\n","            pixel_masks = self.group[f\"{record_id}/human_pixel_masks\"][()]\n","        else: \n","            pixel_masks = None\n","        return bands, pixel_masks\n","    \n","\n","def get_dataloader_hdf5(path, data_type, batch_size, transform=None):\n","    dataset = ContrailDatasetHDF5(path, data_type=data_type, transform=transform)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=1)\n","    return dataloader"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T10:57:34.508338Z","iopub.status.busy":"2023-05-14T10:57:34.507477Z","iopub.status.idle":"2023-05-14T10:57:34.515628Z","shell.execute_reply":"2023-05-14T10:57:34.513762Z","shell.execute_reply.started":"2023-05-14T10:57:34.508274Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2000/2000 [00:56<00:00, 35.71it/s]\n"]}],"source":["path = HDF_DIR + \"/Google_Contrails.hdf5\"\n","train_dataloader_hdf5 = get_dataloader_hdf5(path, 'train', batch_size=1)\n","for bands, masks in tqdm(train_dataloader_hdf5):\n","    continue\n","        # break"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2000/2000 [00:44<00:00, 44.94it/s]\n"]}],"source":["path = HDF_DIR + \"/Google_Contrails.hdf5\"\n","train_dataloader_hdf5 = get_dataloader_hdf5(path, 'train', batch_size=1)\n","for bands, masks in tqdm(train_dataloader_hdf5):\n","    continue\n","        # break"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Results\n","As you can see, the speedup is by a factor of 1.5. I'm confident we can do much better, and there are many things to improve. But the HDF5 file isn't just faster in reading. We can do our preprocessing to save even more time, and the bands are now already concatenated\n","<br><br>\n","It would be great if somebody could tell me if there is a way to upload the full HDF5-file (300+GB) to kaggle. Otherwise, I will share a link to my Google Drive. <br><br><br>\n","PS: If you want to save the dataset folder of this notebook, use: <br>\n","!kaggle datasets create -p \"/kaggle/dataset_train/hdf5/\" --dir-mode zip"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
