{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "# https://chat.openai.com/share/5494b0e3-3a65-4bf3-9581-4eb52dbffe3f\n",
    "class ThreeDConvolutionBlock(torch.nn.Sequential):\n",
    "    def __init__(\n",
    "        self, input_channels: int, output_channels: int, kernel_size: tuple[int, int, int], padding: tuple[int, int, int]\n",
    "    ):\n",
    "        super().__init__(\n",
    "            torch.nn.Conv3d(input_channels, output_channels, kernel_size, padding=padding, padding_mode=\"replicate\"),\n",
    "            torch.nn.BatchNorm3d(output_channels),\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "class TwoPointFiveDSegmentor(torch.nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "        self.F = 3\n",
    "        self.unet_backbone = smp.Unet(...)\n",
    "        \n",
    "        # エンコーダの出力チャンネル用の3D畳み込みブロックを作成（最初のチャンネルを除く）。\n",
    "        three_d_conv_blocks = [\n",
    "            torch.nn.Sequential(\n",
    "                ThreeDConvolutionBlock(channel, channel, (2, 3, 3), (0, 1, 1)), \n",
    "                ThreeDConvolutionBlock(channel, channel, (2, 3, 3), (0, 1, 1))\n",
    "            )\n",
    "            for channel in self.unet_backbone.encoder.out_channels[1:]\n",
    "        ]\n",
    "        self.three_d_convs = torch.nn.ModuleList(three_d_conv_blocks)\n",
    "\n",
    "    def convert_to_2d(self, three_d_conv_block: torch.nn.Module, feature_map: torch.Tensor) -> torch.Tensor:\n",
    "        BxF, C, H, W = feature_map.shape\n",
    "        feature_3d = feature_map.reshape(BxF // self.F, self.F, C, H, W)\n",
    "        feature_3d_transposed = feature_3d.transpose(1, 2) # (B, F, C, H, W)\n",
    "        output = three_d_conv_block(feature_3d_transposed)#(B, F-2, H, W) \n",
    "        output = output.squeeze(2) #(B, C, H, W) \n",
    "        return output \n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, F, H, W = input_tensor.shape\n",
    "        \n",
    "        # U-Netのバックボーンに渡すための入力テンソルの形状を変更。\n",
    "        # 入力テンソルの形状は (B, C, F, H, W) です。\n",
    "        # transpose(1, 2) を使用して、channels と F の次元を入れ替えることでテンソルの形状を (B, F, C, H, W) に変更します。\n",
    "        # 次に、reshape関数を使って、バッチの次元とフレームの次元を結合します。これにより、テンソルの形状は (B * F, C, H, W) に変更されます。\n",
    "        reshaped_input = input_tensor.transpose(1, 2).reshape(B * F, C, H, W)\n",
    "\n",
    "        # 入力テンソルの形状がU-Netと互換性があることを確認。\n",
    "        self.unet_backbone.check_input_shape(reshaped_input)\n",
    "\n",
    "        # 入力テンソルをU-Netエンコーダに渡す。\n",
    "        encoder_features = self.unet_backbone.encoder(reshaped_input)\n",
    "\n",
    "        # エンコーダからの3D特徴をThreeDConvolutionBlockなどを通して2Dに変換。\n",
    "        encoder_features[1:] = [self.convert_to_2d(three_d_conv, feature) for three_d_conv, feature in zip(self.three_d_convs, encoder_features[1:])]\n",
    "\n",
    "        # 変換された特徴をU-Netデコーダに渡す。\n",
    "        decoder_output = self.unet_backbone.decoder(*encoder_features)\n",
    "\n",
    "        # U-Netのセグメンテーションヘッドを使用してセグメンテーションマスクを生成。\n",
    "        segmentation_masks = self.unet_backbone.segmentation_head(decoder_output)\n",
    "        \n",
    "        return segmentation_masks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
